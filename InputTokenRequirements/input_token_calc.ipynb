{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60297d6",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1a3231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install tokenizers\n",
    "%pip install tqdm\n",
    "%pip install pandas\n",
    "%pip install ipywidgets\n",
    "%pip install torch\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141ca2f",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "486b4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "REPO_URL = \"https://github.com/sybaik1/CodeComplex-Data.git\"\n",
    "REPO_DIR = \"CodeComplex-Data\"\n",
    "\n",
    "# Models I would run if I had $3000\n",
    "# MODELS = {\n",
    "#     \"zai-org/GLM-5\",\n",
    "#     \"moonshotai/Kimi-K2.5\",\n",
    "#     \"MiniMaxAI/MiniMax-M2.5\",\n",
    "#     \"stepfun-ai/Step-3.5-Flash\",\n",
    "#     \"zai-org/GLM-4.7\",\n",
    "#     \"XiaomiMiMo/MiMo-V2-Flash\",\n",
    "#     \"deepseek-ai/DeepSeek-V3.2-Speciale\",\n",
    "# }\n",
    "\n",
    "MODELS = {\n",
    "    \"Qwen/Qwen3-8B\", # 8B params\n",
    "    \"Qwen/Qwen2.5-Coder-7B-Instruct\", # 7B params\n",
    "    \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\", # 8B params\n",
    "    \"deepseek-ai/deepseek-coder-6.7b-instruct\", # 6.7B params\n",
    "    \"deepseek-ai/deepseek-coder-1.3b-instruct\", # 1.3B params\n",
    "    \"bigcode/starcoder2-7b\", # 7B params\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\", # 8B params\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", # 3B params\n",
    "    \"openai/gpt-oss-20b\", # 20B params\n",
    "    # \"meta-llama/CodeLlama-7b-Instruct-hf\", # 7B params\n",
    "}\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a Software Engineer that is meant to analyze the time complexity of a given code snippet. You will be given a code snippet in Java or Python, and you will analyze the time complexity of the snippet. You're output must be in the form of a JSON with the following format:\n",
    "{{\n",
    "    \"time_complexity\": \"[Your time complexity class guess here]\",\n",
    "    \"explanation\": \"[Brief 20 word explanation of your guess]\"\n",
    "}}\n",
    "Your time complexity class guess must be one of the following: \"constant\", \"logn\", \"linear\", \"nlogn\", \"quadratic\", \"cubic\", or \"exponential\". Additionally, your explanation must be brief and concise, and should NOT EXCEED 20 WORDS.\n",
    "DO NOT INCLUDE ANYTHING OTHER THAN THE JSON IN YOUR RESPONSE.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c3133",
   "metadata": {},
   "source": [
    "# Importing dataset (CodeComplex)\n",
    "GitHub Repo: https://github.com/sybaik1/CodeComplex-Data.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6a5f7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits: ['java', 'python'] with total examples: 9800\n"
     ]
    }
   ],
   "source": [
    "JSONL_FILES = {\n",
    "    \"java\": os.path.join(REPO_DIR, \"java_data.jsonl\"),\n",
    "    \"python\": os.path.join(REPO_DIR, \"python_data.jsonl\"),\n",
    "}\n",
    "\n",
    "# Sanity Check\n",
    "missing = [k for k, p in JSONL_FILES.items() if not os.path.exists(p)]\n",
    "if missing:\n",
    "    raise FileNotFoundError(f\"Missing JSONL files for: {', '.join(missing)}. Please ensure the repository is cloned and files are in place.\")\n",
    "\n",
    "dataset_dict = load_dataset(\n",
    "    \"json\",\n",
    "    data_files=JSONL_FILES,\n",
    ")\n",
    "\n",
    "total_examples = sum(len(ds) for ds in dataset_dict.values())\n",
    "print(f\"Loaded splits: {list(dataset_dict.keys())} with total examples: {total_examples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c473b7",
   "metadata": {},
   "source": [
    "# Tokenize and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12dc4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(model_name):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to load tokenizer for {model_name}, falling back to GPT2 tokenizer.\\nError: {e}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True, trust_remote_code=True)\n",
    "        if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        return tokenizer\n",
    "\n",
    "def extract_code_snippet(example):\n",
    "    val = example.get(\"src\")\n",
    "    if isinstance(val, str) and val.strip():\n",
    "        return val\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4cf0210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: meta-llama/Llama-3.1-8B-Instruct ===\n",
      "Unable to load tokenizer for meta-llama/Llama-3.1-8B-Instruct, falling back to GPT2 tokenizer.\n",
      "Error: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct.\n",
      "401 Client Error. (Request ID: Root=1-69931f0d-35accf8e7df4ecf67fae66ff;868269f0-052e-4dfa-9f81-0072bf32134d)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.1-8B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for meta-llama/Llama-3.1-8B-Instruct:   0%|          | 0/4900 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1383 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing for meta-llama/Llama-3.1-8B-Instruct: 100%|██████████| 4900/4900 [00:05<00:00, 912.25it/s] \n",
      "Tokenizing for meta-llama/Llama-3.1-8B-Instruct: 100%|██████████| 4900/4900 [00:02<00:00, 1904.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: Qwen/Qwen2.5-Coder-7B-Instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for Qwen/Qwen2.5-Coder-7B-Instruct:  84%|████████▎ | 4100/4900 [00:03<00:00, 819.77it/s] Token indices sequence length is longer than the specified maximum sequence length for this model (48562 > 32768). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing for Qwen/Qwen2.5-Coder-7B-Instruct: 100%|██████████| 4900/4900 [00:05<00:00, 979.42it/s]\n",
      "Tokenizing for Qwen/Qwen2.5-Coder-7B-Instruct: 100%|██████████| 4900/4900 [00:02<00:00, 1910.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: openai/gpt-oss-20b ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for openai/gpt-oss-20b: 100%|██████████| 4900/4900 [00:04<00:00, 1021.34it/s]\n",
      "Tokenizing for openai/gpt-oss-20b: 100%|██████████| 4900/4900 [00:02<00:00, 2069.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: meta-llama/Llama-3.2-3B-Instruct ===\n",
      "Unable to load tokenizer for meta-llama/Llama-3.2-3B-Instruct, falling back to GPT2 tokenizer.\n",
      "Error: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct.\n",
      "401 Client Error. (Request ID: Root=1-69931f27-3d66a6023e3fc4ed714f6f01;b0dc375f-2d4d-4ce9-9ed6-48f173801a64)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-3.2-3B-Instruct is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for meta-llama/Llama-3.2-3B-Instruct:   0%|          | 0/4900 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1383 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing for meta-llama/Llama-3.2-3B-Instruct: 100%|██████████| 4900/4900 [00:05<00:00, 895.77it/s] \n",
      "Tokenizing for meta-llama/Llama-3.2-3B-Instruct: 100%|██████████| 4900/4900 [00:02<00:00, 1873.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: deepseek-ai/deepseek-coder-6.7b-instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for deepseek-ai/deepseek-coder-6.7b-instruct:  62%|██████▏   | 3037/4900 [00:04<00:03, 479.43it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (20069 > 16384). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing for deepseek-ai/deepseek-coder-6.7b-instruct: 100%|██████████| 4900/4900 [00:09<00:00, 520.96it/s]\n",
      "Tokenizing for deepseek-ai/deepseek-coder-6.7b-instruct: 100%|██████████| 4900/4900 [00:04<00:00, 1131.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_parameters` for 'rope_type'='yarn': {'attn_factor'}\n",
      "Tokenizing for deepseek-ai/DeepSeek-R1-0528-Qwen3-8B: 100%|██████████| 4900/4900 [00:05<00:00, 937.00it/s] \n",
      "Tokenizing for deepseek-ai/DeepSeek-R1-0528-Qwen3-8B: 100%|██████████| 4900/4900 [00:02<00:00, 1835.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: Qwen/Qwen3-8B ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for Qwen/Qwen3-8B: 100%|██████████| 4900/4900 [00:05<00:00, 940.46it/s] \n",
      "Tokenizing for Qwen/Qwen3-8B: 100%|██████████| 4900/4900 [00:02<00:00, 1848.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: bigcode/starcoder2-7b ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for bigcode/starcoder2-7b: 100%|██████████| 4900/4900 [00:05<00:00, 939.68it/s] \n",
      "Tokenizing for bigcode/starcoder2-7b: 100%|██████████| 4900/4900 [00:02<00:00, 1895.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing model: deepseek-ai/deepseek-coder-1.3b-instruct ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing for deepseek-ai/deepseek-coder-1.3b-instruct:  62%|██████▏   | 3037/4900 [00:04<00:03, 495.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (20069 > 16384). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing for deepseek-ai/deepseek-coder-1.3b-instruct: 100%|██████████| 4900/4900 [00:09<00:00, 531.56it/s]\n",
      "Tokenizing for deepseek-ai/deepseek-coder-1.3b-instruct: 100%|██████████| 4900/4900 [00:04<00:00, 1165.01it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for model_name in MODELS:\n",
    "    print(f\"\\n=== Processing model: {model_name} ===\")\n",
    "    tokenizer = get_tokenizer(model_name)\n",
    "\n",
    "    token_counts = []\n",
    "    skipped = 0\n",
    "\n",
    "    for ds in dataset_dict.values():\n",
    "        for example in tqdm(ds, desc=f\"Tokenizing for {model_name}\", total=len(ds)):\n",
    "            code_snippet = extract_code_snippet(example)\n",
    "            if not code_snippet:\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            prompt = SYSTEM_PROMPT + \"\\n\\n\" + code_snippet\n",
    "            input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            token_counts.append(len(input_ids))\n",
    "\n",
    "    if not token_counts:\n",
    "        print(f\"No valid code snippets found for {model_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    total = int(sum(token_counts))\n",
    "    avg = float(total / len(token_counts)) if token_counts else 0\n",
    "\n",
    "    results.append({\n",
    "        \"model\": model_name,\n",
    "        \"total_tokens\": total,\n",
    "        \"avg_tokens\": avg,\n",
    "        \"skipped\": skipped,\n",
    "        \"counted\": len(token_counts)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "523b2911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "Model: meta-llama/Llama-3.1-8B-Instruct, Total Tokens: 11772544, Average Tokens: 1201.28, Skipped: 0, Counted: 9800\n",
      "Model: Qwen/Qwen2.5-Coder-7B-Instruct, Total Tokens: 6840326, Average Tokens: 697.9924489795918, Skipped: 0, Counted: 9800\n",
      "Model: openai/gpt-oss-20b, Total Tokens: 6869918, Average Tokens: 701.0120408163265, Skipped: 0, Counted: 9800\n",
      "Model: meta-llama/Llama-3.2-3B-Instruct, Total Tokens: 11772544, Average Tokens: 1201.28, Skipped: 0, Counted: 9800\n",
      "Model: deepseek-ai/deepseek-coder-6.7b-instruct, Total Tokens: 8760083, Average Tokens: 893.8860204081633, Skipped: 0, Counted: 9800\n",
      "Model: deepseek-ai/DeepSeek-R1-0528-Qwen3-8B, Total Tokens: 6840326, Average Tokens: 697.9924489795918, Skipped: 0, Counted: 9800\n",
      "Model: Qwen/Qwen3-8B, Total Tokens: 6840326, Average Tokens: 697.9924489795918, Skipped: 0, Counted: 9800\n",
      "Model: bigcode/starcoder2-7b, Total Tokens: 7667566, Average Tokens: 782.404693877551, Skipped: 0, Counted: 9800\n",
      "Model: deepseek-ai/deepseek-coder-1.3b-instruct, Total Tokens: 8760083, Average Tokens: 893.8860204081633, Skipped: 0, Counted: 9800\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Final Results ===\")\n",
    "for result in results:\n",
    "    print(f\"Model: {result['model']}, Total Tokens: {result['total_tokens']}, Average Tokens: {result['avg_tokens']}, Skipped: {result['skipped']}, Counted: {result['counted']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5fa4518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>avg_tokens</th>\n",
       "      <th>skipped</th>\n",
       "      <th>counted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meta-llama/Llama-3.1-8B-Instruct</td>\n",
       "      <td>11772544</td>\n",
       "      <td>1201.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen/Qwen2.5-Coder-7B-Instruct</td>\n",
       "      <td>6840326</td>\n",
       "      <td>697.992449</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>openai/gpt-oss-20b</td>\n",
       "      <td>6869918</td>\n",
       "      <td>701.012041</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>meta-llama/Llama-3.2-3B-Instruct</td>\n",
       "      <td>11772544</td>\n",
       "      <td>1201.280000</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepseek-ai/deepseek-coder-6.7b-instruct</td>\n",
       "      <td>8760083</td>\n",
       "      <td>893.886020</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>deepseek-ai/DeepSeek-R1-0528-Qwen3-8B</td>\n",
       "      <td>6840326</td>\n",
       "      <td>697.992449</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Qwen/Qwen3-8B</td>\n",
       "      <td>6840326</td>\n",
       "      <td>697.992449</td>\n",
       "      <td>0</td>\n",
       "      <td>9800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      model  total_tokens   avg_tokens  \\\n",
       "0          meta-llama/Llama-3.1-8B-Instruct      11772544  1201.280000   \n",
       "1            Qwen/Qwen2.5-Coder-7B-Instruct       6840326   697.992449   \n",
       "2                        openai/gpt-oss-20b       6869918   701.012041   \n",
       "3          meta-llama/Llama-3.2-3B-Instruct      11772544  1201.280000   \n",
       "4  deepseek-ai/deepseek-coder-6.7b-instruct       8760083   893.886020   \n",
       "5     deepseek-ai/DeepSeek-R1-0528-Qwen3-8B       6840326   697.992449   \n",
       "6                             Qwen/Qwen3-8B       6840326   697.992449   \n",
       "\n",
       "   skipped  counted  \n",
       "0        0     9800  \n",
       "1        0     9800  \n",
       "2        0     9800  \n",
       "3        0     9800  \n",
       "4        0     9800  \n",
       "5        0     9800  \n",
       "6        0     9800  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(results)\n",
    "output_df.to_csv(\"token_counts.csv\", index=False)\n",
    "output_df.head(7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
